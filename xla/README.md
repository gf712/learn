# Tensorflow's XLA

I recently took some interest in JIT compilers, so I wanted to see how easy/difficult it would
be to integrate Tensorflow's XLA. Turns out PyTorch uses XLA too so they have a 
[repo](https://github.com/pytorch/xla) to optimise their own graphs at runtime.
Another helpful (maybe outdated?) resource is [this](https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html1)
 documentation/tutorial. 
 
 Here I just use a local installation of the PyTorch XLA repo and link to it.
 This could be done directly with tf, but I found it easier to link to the shared library
 generated by PyTorch's XLA project.
 
 To reproduce the results:
 ```bash
git clone https://github.com/pytorch/xla.git ~/xla_torch/
cd ~/xla_torch
./build_torch_xla_libs.sh

# go to this repo
...
mkdir build && cd build
cmake ..
make
./xla
```